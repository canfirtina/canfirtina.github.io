---
---

@article{firtina_aphmm_2022,
  title = {{ApHMM}: {A} {Profile} {Hidden} {Markov} {Model} {Acceleration} {Framework} for {Genome} {Analysis}},
  copyright = {Creative Commons Attribution 4.0 International},
  journal = {arXiv},
  author = {Firtina, Can and Pillai, Kamlesh and Kalsi, Gurpreet S. and Suresh, Bharathwaj and Senol Cali, Damla and Kim, Jeremie S. and Shahroodi, Taha and Cavlak, Meryem Banu and Lindegger, Joel and Alser, Mohammed and G{\'o}mez-Luna, Juan and Subramoney, Sreenivas and Mutlu, Onur},
  year = {2022},
  abstract = {Profile hidden Markov models (pHMMs) are widely used in many bioinformatics applications as they can capture differences between biological sequences (e.g., DNA or protein sequences) to accurately identify their similarities. PHMMs represent sequences with a graph structure such that their states and edges are associated with probabilities to account for changes within sequences, which is then used to calculate a similarity score of a sequence compared to a pHMM graph. Accurately setting the probabilities in pHMMs is essential to correctly identify similarities between sequences. To achieve this, the Baum-Welch algorithm is a commonly-used and highly-accurate method that can change the probabilities of a pHMM to maximize the similarity score of a provided set of input sequences. However, the Baum-Welch algorithm is computationally expensive and existing works accelerate the Baum-Welch algorithm only for the widely-adopted traditional design of pHMMs, which cannot support all the bioinformatics applications using pHMMs.
  We propose ApHMM, the first hardware-software co-design framework to accelerate the Baum-Welch algorithm for a wide range of pHMM-based applications. ApHMM 1) uses a modified design of traditional pHMMs that can support the pHMM-based applications, 2) reduces the computational and data movement overheads of the Baum-Welch algorithm with software-level optimizations, and 3) provides a hardware design to accelerate the execution of the Baum-Welch algorithm for the modified design of pHMMs. We implement our software optimizations on both CPUs and GPUs, and hardware-software optimizations on a specialized hardware to provide an acceleration framework for all the pHMM-based applications. Our evaluation using a wide range of applications shows that ApHMM outperforms the state-of-the-art CPU implementations of three use cases 1) error correction, 2) protein family search, and 3) multiple sequence alignment by 59.94×, 1.75×, and 1.95×, respectively, while improving their energy efficiency by 64.24×, 1.75×, and 1.96×. When compared to CPU, GPU, and FPGA implementations, ApHMM accelerates the Baum-Welch computation by 15.55×-260.03×, 1.83×-5.34×, and 27.97×, respectively.},
  selected=true,
  montha={May},
  abbr={arXiv},
  bibtex_show=true,
}

@article{alser_packaging_2022,
  title = {Packaging, containerization, and virtualization of computational omics methods: {Advances}, challenges, and opportunities},
  copyright = {Creative Commons Attribution 4.0 International},
  url = {https://arxiv.org/abs/2203.16261},
  journal = {arXiv},
  abstract = {Omics software tools have reshaped the landscape of modern biology and become an essential component of biomedical research. The increasing dependence of biomedical scientists on these powerful tools creates a need for easier installation and greater usability. Packaging, virtualization, and containerization are different approaches to satisfy this need by wrapping omics tools in additional software that makes the omics tools easier to install and use. Here, we systematically review practices across prominent packaging, virtualization, and containerization platforms. We outline the challenges, advantages, and limitations of each approach and some of the most widely used platforms from the perspectives of users, software developers, and system administrators. We also propose principles to make packaging, virtualization, and containerization of omics software more sustainable and robust to increase the reproducibility of biomedical and life science research.},
  author = {Alser, Mohammed and Waymost, Sharon and Ayyala, Ram and Lawlor, Brendan and Abdill, Richard J. and Rajkumar, Neha and LaPierre, Nathan and Brito, Jaqueline and Ribeiro-dos-Santos, Andre M. and Firtina, Can and Almadhoun Alserr, Nour and Sarwal, Varuni and Eskin, Eleazar and Hu, Qiyang and Strong, Derek and Byoung-Do and {Kim} and Abedalthagafi, Malak S. and Mutlu, Onur and Mangul, Serghei},
  year = {2022},
  montha={March},
  doi = {10.48550/ARXIV.2203.16261},
  pdf={alser_2022_arxiv_packaging,_containerization,_and_virtualization_of_computational_omics_methods.pdf},
  abbr={arXiV},
  bibtex_show=true,
}

@inproceedings{mansouri_ghiasi_genstore_2022,
  address = {New York, NY, USA},
  series = {{ASPLOS} 2022},
  title = {{GenStore}: {A} {High}-{Performance} in-{Storage} {Processing} {System} for {Genome} {Sequence} {Analysis}},
  copyright = {All rights reserved},
  isbn = {978-1-4503-9205-1},
  url = {https://doi.org/10.1145/3503222.3507702},
  confurl = {https://asplos-conference.org/2022/},
  doi = {10.1145/3503222.3507702},
  abstract = {Read mapping is a fundamental step in many genomics applications. It is used to identify potential matches and differences between fragments (called reads) of a sequenced genome and an already known genome (called a reference genome). Read mapping is costly because it needs to perform approximate string matching (ASM) on large amounts of data. To address the computational challenges in genome analysis, many prior works propose various approaches such as accurate filters that select the reads within a dataset of genomic reads (called a read set) that must undergo expensive computation, efficient heuristics, and hardware acceleration. While effective at reducing the amount of expensive computation, all such approaches still require the costly movement of a large amount of data from storage to the rest of the system, which can significantly lower the end-to-end performance of read mapping in conventional and emerging genomics systems. We propose GenStore, the first in-storage processing system designed for genome sequence analysis that greatly reduces both data movement and computational overheads of genome sequence analysis by exploiting low-cost and accurate in-storage filters. GenStore leverages hardware/software co-design to address the challenges of in-storage processing, supporting reads with 1)\&nbsp;different properties such as read lengths and error rates, which highly depend on the sequencing technology, and 2)\&nbsp;different degrees of genetic variation compared to the reference genome, which highly depends on the genomes that are being compared. Through rigorous analysis of read mapping processes of reads with different properties and degrees of genetic variation, we meticulously design low-cost hardware accelerators and data/computation flows inside a NAND flash-based solid-state drive (SSD). Our evaluation using a wide range of real genomic datasets shows that GenStore, when implemented in three modern NAND flash-based SSDs, significantly improves the read mapping performance of state-of-the-art software (hardware) baselines by 2.07-6.05× (1.52-3.32×) for read sets with high similarity to the reference genome and 1.45-33.63× (2.70-19.2×) for read sets with low similarity to the reference genome.},
  booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
  publisher = {Association for Computing Machinery},
  author = {Mansouri Ghiasi, Nika and Park, Jisung and Mustafa, Harun and Kim, Jeremie and Olgun, Ataberk and Gollwitzer, Arvid and Senol Cali, Damla and Firtina, Can and Mao, Haiyu and Almadhoun Alserr, Nour and Ausavarungnirun, Rachata and Vijaykumar, Nandita and Alser, Mohammed and Mutlu, Onur},
  year = {2022},
  montha={March},
  keywords = {Genomics, Filtering, Near-Data Processing, Read Mapping, Storage},
  pages = {635--654},
  pdf={mansouri_ghiasi_2022_association_for_computing_machinery_genstore.pdf},
  code={https://github.com/CMU-SAFARI/GenStore},
  abbr={ASPLOS},
  slides={https://people.inf.ethz.ch/omutlu/pub/GenStore_asplos22-talk.pdf},
  pptx={https://people.inf.ethz.ch/omutlu/pub/GenStore_asplos22-talk.pptx},
  video={https://www.youtube.com/watch?v=bv7hgXOOMjk},
  bibtex_show=true,
}

@article{kim_fastremap_2022,
  title = {{FastRemap}: {A} {Tool} for {Quickly} {Remapping} {Reads} between {Genome} {Assemblies}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url = {https://arxiv.org/abs/2201.06255},
  journal = {arXiv},
  abstract = {A genome read data set can be quickly and efficiently remapped from one reference to another similar reference (e.g., between two reference versions or two similar species) using a variety of tools, e.g., the commonly-used CrossMap tool. With the explosion of available genomic data sets and references, high-performance remapping tools will be even more important for keeping up with the computational demands of genome assembly and analysis. We provide FastRemap, a fast and efficient tool for remapping reads between genome assemblies. FastRemap provides up to a 7.19× speedup (5.97×, on average) and uses as low as 61.7% (80.7%, on average) of the peak memory consumption compared to the state-of-the-art remapping tool, CrossMap.},
  author = {Kim, Jeremie S. and Firtina, Can and Cavlak, Meryem Banu and Senol Cali, Damla and Alkan, Can and Mutlu, Onur},
  year = {2022},
  montha={January},
  doi = {10.48550/ARXIV.2201.06255},
  pdf={kim_2022_arxiv_fastremap.pdf},
  code={https://github.com/CMU-SAFARI/FastRemap},
  abbr={arXiv},
  bibtex_show=true,
}

@article{firtina_blend_2021,
  title = {{BLEND}: {A} {Fast}, {Memory}-{Efficient}, and {Accurate} {Mechanism} to {Find} {Fuzzy} {Seed} {Matches}},
  copyright = {All rights reserved},
  url = {https://arxiv.org/abs/2112.08687},
  journal = {arXiv},
  abstract = {Comparing genomic sequences is a fundamental yet costly step in most genomic analyses. It requires identifying similar genomic sequence pairs sharing a sufficient number of short subsequences, called seeds. The length and number of seed matches between sequences directly impact the sensitivity, performance, and memory footprint of today's read mappers. Existing attempts optimizing seed matches suffer from either 1) increasing the use of the costly sequence alignment step due to finding a large number of sequence pairs for alignment or 2) limited sensitivity when finding seed matches. Our goal is to efficiently and effectively identify potentially similar sequences using a small number of seed matches, while tolerating variations within each seed. To this end, we introduce BLEND, a fast, memory-efficient, and accurate mechanism to find approximate (i.e., fuzzy) seed matches. BLEND 1) finds the minimizer k-mers and extends them to increase the length of seeds and 2) utilizes the SimHash technique to enable generating the same hash values for highly similar seeds, which allows matching fuzzy seeds with a single-look up. We show the benefits of BLEND when used in two important genomics applications: read overlapping and read mapping. For read overlapping, BLEND enables a 2.6x-63.5x (on average 19.5x) faster and 0.9x-9.7x (on average 3.6x) more memory-efficient implementation than the state-of-the-art tool, Minimap2. We observe that BLEND finds better quality overlaps that lead to more accurate de novo assemblies compared to Minimap2. For read mapping, BLEND provides 0.7x-3.7x (on average 1.7x) speedup compared to Minimap2.},
  author = {Firtina, Can and Park, Jisung and Alser, Mohammed and Kim, Jeremie S. and Senol Cali, Damla and Shahroodi, Taha and Ghiasi, Nika Mansouri and Singh, Gagandeep and Kanellopoulos, Konstantinos and Alkan, Can and Mutlu, Onur},
  year = {2021},
  montha={December},
  doi = {10.48550/ARXIV.2112.08687},
  selected=true,
  pdf={firtina_2021_arxiv_blend.pdf},
  code={https://github.com/CMU-SAFARI/BLEND},
  abbr={arXiv},
  bibtex_show=true,
}

@article{onural_modeling_2021,
  title = {Modeling {Economic} {Activities} and {Random} {Catastrophic} {Failures} of {Financial} {Networks} via {Gibbs} {Random} {Fields}},
  volume = {58},
  issn = {1572-9974},
  url = {https://doi.org/10.1007/s10614-020-10023-3},
  doi = {10.1007/s10614-020-10023-3},
  abstract = {The complicated economic behavior of entities in a population can be modeled as a Gibbs random field (GRF). Even with simple GRF models, which restrict direct statistical interactions with a small number of neighbors of an entity, real life economic and financial activities may be effectively described. A computer simulator is developed to run empirical experiments to assess different coupling structures and parameters of the presented model; it is possible to test many economic and financial models and policies in terms of their transient and steady-state consequences.},
  number = {2},
  journal = {Computational Economics},
  author = {Onural, Levent and Pınar, Mustafa Çelebi and Firtina, Can},
  montha={August},
  year = {2021},
  pages = {203--232},
  pdf={onural_2021_modeling_economic_activities_and_random_catastrophic_failures_of_financial.pdf},
  bibtex_show=true,
}

@article{kim_airlift_2021,
  title = {{AirLift}: {A} {Fast} and {Comprehensive} {Technique} for {Remapping} {Alignments} between {Reference} {Genomes}},
  copyright = {All rights reserved},
  url = {http://biorxiv.org/content/early/2021/02/17/2021.02.16.431517.abstract},
  doi = {10.1101/2021.02.16.431517},
  abstract = {As genome sequencing tools and techniques improve, researchers are able to incrementally assemble more accurate reference genomes, which enable sensitivity in read mapping and downstream analysis such as variant calling. A more sensitive downstream analysis is critical for a better understanding of the genome donor (e.g., health characteristics). Therefore, read sets from sequenced samples should ideally be mapped to the latest available reference genome that represents the most relevant population. Unfortunately, the increasingly large amount of available genomic data makes it prohibitively expensive to fully re-map each read set to its respective reference genome every time the reference is updated. There are several tools that attempt to accelerate the process of updating a read data set from one reference to another (i.e., remapping) by 1) identifying regions that appear similarly between two references and 2) updating the mapping location of reads that map to any of the identified regions in the old reference to the corresponding similar region in the new reference. The main drawback of existing approaches is that if a read maps to a region in the old reference that does not appear with a reasonable degree of similarity in the new reference, the read cannot be remapped. We find that, as a result of this drawback, a significant portion of annotations (i.e., coding regions in a genome) are lost when using state-of-the-art remapping tools. To address this major limitation in existing tools, we propose AirLift, a fast and comprehensive technique for remapping alignments from one genome to another. Compared to the state-of-the-art method for remapping reads (i.e., full mapping), AirLift reduces 1) the number of reads (out of the entire read set) that need to be fully mapped to the new reference by up to 99.99\% and 2) the overall execution time to remap read sets between two reference genome versions by 6.7×, 6.6×, and 2.8× for large (human), medium (C. elegans), and small (yeast) reference genomes, respectively. We validate our remapping results with GATK and find that AirLift provides similar accuracy in identifying ground truth SNP and INDEL variants as the baseline of fully mapping a read set.Code Availability AirLift source code and readme describing how to reproduce our results are available at https://github.com/CMU-SAFARI/AirLift.Competing Interest StatementThe authors have declared no competing interest.},
  journal = {bioRxiv},
  author = {Kim, Jeremie S. and Firtina, Can and Cavlak, Meryem Banu and Cali, Damla Senol and Hajinazar, Nastaran and Alser, Mohammed and Alkan, Can and Mutlu, Onur},
  month = {January},
  year = {2021},
  pages = {2021.02.16.431517},
  selected=true,
  pdf={kim_2019_arxiv_airlift.pdf},
  code={https://github.com/CMU-SAFARI/AirLift},
  abbr={bioRxiv},
  bibtex_show=true,
}

@inproceedings{cali_genasm_2020,
  address = {Virtual},
  title = {{GenASM}: {A} {High}-{Performance}, {Low}-{Power} {Approximate} {String} {Matching} {Acceleration} {Framework} for {Genome} {Sequence} {Analysis}},
  copyright = {All rights reserved},
  url = {https://ieeexplore.ieee.org/document/9251930},
  confurl = {https://www.microarch.org/micro53/},
  doi = {10.1109/MICRO50266.2020.00081},
  abstract = {Genome sequence analysis has enabled significant advancements in medical and scientific areas such as personalized medicine, outbreak tracing, and the understanding of evolution. To perform genome sequencing, devices extract small random fragments of an organism's DNA sequence (known as reads). The first step of genome sequence analysis is a computational process known as read mapping. In read mapping, each fragment is matched to its potential location in the reference genome with the goal of identifying the original location of each read in the genome. Unfortunately, rapid genome sequencing is currently bottlenecked by the computational power and memory bandwidth limitations of existing systems, as many of the steps in genome sequence analysis must process a large amount of data. A major contributor to this bottleneck is approximate string matching (ASM), which is used at multiple points during the mapping process. ASM enables read mapping to account for sequencing errors and genetic variations in the reads. We propose GenASM, the first ASM acceleration framework for genome sequence analysis. GenASM performs bitvectorbased ASM, which can efficiently accelerate multiple steps of genome sequence analysis. We modify the underlying ASM algorithm (Bitap) to significantly increase its parallelism and reduce its memory footprint. Using this modified algorithm, we design the first hardware accelerator for Bitap. Our hardware accelerator consists of specialized systolic-array-based compute units and on-chip SRAMs that are designed to match the rate of computation with memory capacity and bandwidth, resulting in an efficient design whose performance scales linearly as we increase the number of compute units working in parallel. We demonstrate that GenASM provides significant performance and power benefits for three different use cases in genome sequence analysis. First, GenASM accelerates read alignment for both long reads and short reads. For long reads, GenASM outperforms state-of-the-art software and hardware accelerators by 116× and 3.9×, respectively, while reducing power consumption by 37× and 2.7×. For short reads, GenASM outperforms state-of-the-art software and hardware accelerators by 111× and 1.9×. Second, GenASM accelerates pre-alignment filtering for short reads, with 3.7× the performance of a state-of-the-art pre-alignment filter, while reducing power consumption by 1.7× and significantly improving the filtering accuracy. Third, GenASM accelerates edit distance calculation, with 22-12501× and 9.3-400× speedups over the state-of-the-art software library and FPGA-based accelerator, respectively, while reducing power consumption by 548-582× and 67×. We conclude that GenASM is a flexible, high-performance, and low-power framework, and we briefly discuss four other use cases that can benefit from GenASM.},
  booktitle = {Proceedings of the 53rd {International} {Symposium} on {Microarchitecture}},
  author = {Senol Cali, Damla and Kalsi, Gurpreet S. and Bingöl, Zülal and Firtina, Can and Subramanian, Lavanya and Kim, Jeremie S. and Ausavarungnirun, Rachata and Alser, Mohammed and G{\'o}mez-Luna, Juan and Boroumand, Amirali and Norion, Anant and Scibisz, Allison and Subramoney, Sreenivas and Alkan, Can and Ghose, Saugata and Mutlu, Onur},
  year = {2020},
  montha={October},
  pages = {951--966},
  pdf={cali_2020_genasm.pdf},
  code={https://github.com/CMU-SAFARI/GenASM},
  abbr={MICRO},
  slides={https://people.inf.ethz.ch/omutlu/pub/GenASM-approximate-string-matching-framework-for-genome-analysis_micro20-talk.pdf},
  pptx={https://people.inf.ethz.ch/omutlu/pub/GenASM-approximate-string-matching-framework-for-genome-analysis_micro20-talk.pptx},
  video={http://www.youtube.com/watch?v=XoLpzmN-Pas},
  bibtex_show=true,
}

@article{firtina_apollo_2020,
  title = {Apollo: a sequencing-technology-independent, scalable and accurate assembly polishing algorithm},
  volume = {36},
  issn = {1367-4803},
  url = {https://doi.org/10.1093/bioinformatics/btaa179},
  doi = {10.1093/bioinformatics/btaa179},
  abstract = {Third-generation sequencing technologies can sequence long reads that contain as many as 2 million base pairs. These long reads are used to construct an assembly (i.e. the subject’s genome), which is further used in downstream genome analysis. Unfortunately, third-generation sequencing technologies have high sequencing error rates and a large proportion of base pairs in these long reads is incorrectly identified. These errors propagate to the assembly and affect the accuracy of genome analysis. Assembly polishing algorithms minimize such error propagation by polishing or fixing errors in the assembly by using information from alignments between reads and the assembly (i.e. read-to-assembly alignment information). However, current assembly polishing algorithms can only polish an assembly using reads from either a certain sequencing technology or a small assembly. Such technology-dependency and assembly-size dependency require researchers to (i) run multiple polishing algorithms and (ii) use small chunks of a large genome to use all available readsets and polish large genomes, respectively.We introduce Apollo, a universal assembly polishing algorithm that scales well to polish an assembly of any size (i.e. both large and small genomes) using reads from all sequencing technologies (i.e. second- and third-generation). Our goal is to provide a single algorithm that uses read sets from all available sequencing technologies to improve the accuracy of assembly polishing and that can polish large genomes. Apollo (i) models an assembly as a profile hidden Markov model (pHMM), (ii) uses read-to-assembly alignment to train the pHMM with the Forward–Backward algorithm and (iii) decodes the trained model with the Viterbi algorithm to produce a polished assembly. Our experiments with real readsets demonstrate that Apollo is the only algorithm that (i) uses reads from any sequencing technology within a single run and (ii) scales well to polish large assemblies without splitting the assembly into multiple parts. Source code is available at https://github.com/CMU-SAFARI/Apollo. Supplementary data are available at Bioinformatics online.},
  number = {12},
  urldate = {2022-04-13},
  journal = {Bioinformatics},
  author = {Firtina, Can and Kim, Jeremie S. and Alser, Mohammed and Senol Cali, Damla and Cicek, A Ercument and Alkan, Can and Mutlu, Onur},
  montha={June},
  year = {2020},
  pages = {3669--3679},
  selected=true,
  pdf={firtina_2020_apollo.pdf},
  code={https://github.com/CMU-SAFARI/Apollo},
  abbr={Bioinformatics},
  bibtex_show=true,
}

@article{firtina_hercules_2018,
  title = {Hercules: a profile {HMM}-based hybrid error correction algorithm for long reads},
  volume = {46},
  issn = {0305-1048},
  url = {https://doi.org/10.1093/nar/gky724},
  doi = {10.1093/nar/gky724},
  abstract = {Choosing whether to use second or third generation sequencing platforms can lead to trade-offs between accuracy and read length. Several types of studies require long and accurate reads. In such cases researchers often combine both technologies and the erroneous long reads are corrected using the short reads. Current approaches rely on various graph or alignment based techniques and do not take the error profile of the underlying technology into account. Efficient machine learning algorithms that address these shortcomings have the potential to achieve more accurate integration of these two technologies. We propose Hercules, the first machine learning-based long read error correction algorithm. Hercules models every long read as a profile Hidden Markov Model with respect to the underlying platform’s error profile. The algorithm learns a posterior transition/emission probability distribution for each long read to correct errors in these reads. We show on two DNA-seq BAC clones (CH17-157L1 and CH17-227A2) that Hercules-corrected reads have the highest mapping rate among all competing algorithms and have the highest accuracy when the breadth of coverage is high. On a large human CHM1 cell line WGS data set, Hercules is one of the few scalable algorithms; and among those, it achieves the highest accuracy.},
  number = {21},
  urldate = {2022-04-13},
  journal = {Nucleic Acids Research},
  author = {Firtina, Can and Bar-Joseph, Ziv and Alkan, Can and Cicek, A. Ercument},
  montha={November},
  year = {2018},
  pages = {e125--e125},
  pdf={firtina_2018_hercules.pdf},
  code={https://github.com/BilkentCompGen/hercules},
  abbr={NAR},
  bibtex_show=true,
}

@article{otlu_glanet_2017,
  title = {{GLANET}: genomic loci annotation and enrichment tool},
  volume = {33},
  copyright = {All rights reserved},
  issn = {1367-4803},
  url = {https://doi.org/10.1093/bioinformatics/btx326},
  doi = {10.1093/bioinformatics/btx326},
  abstract = {Genomic studies identify genomic loci representing genetic variations, transcription factor (TF) occupancy, or histone modification through next generation sequencing (NGS) technologies. Interpreting these loci requires evaluating them with known genomic and epigenomic annotations.We present GLANET as a comprehensive annotation and enrichment analysis tool which implements a sampling-based enrichment test that accounts for GC content and/or mappability biases, jointly or separately. GLANET annotates and performs enrichment analysis on these loci with a rich library. We introduce and perform novel data-driven computational experiments for assessing the power and Type-I error of its enrichment procedure which show that GLANET has attained high statistical power and well-controlled Type-I error rate. As a key feature, users can easily extend its library with new gene sets and genomic intervals. Other key features include assessment of impact of single nucleotide variants (SNPs) on TF binding sites and regulation based pathway enrichment analysis.GLANET can be run using its GUI or on command line. GLANET’s source code is available at https://github.com/burcakotlu/GLANET. Tutorials are provided at https://glanet.readthedocs.org.Supplementary data are available at Bioinformatics online.},
  number = {18},
  journal = {Bioinformatics},
  author = {Otlu, Burçak and Firtina, Can and Keleş, Sündüz and Tastan, Oznur},
  montha={September},
  year = {2017},
  pages = {2818--2828},
  pdf={otlu_2017_glanet.pdf},
  code={https://github.com/burcakotlu/GLANET},
  abbr={Bioinformatics},
  bibtex_show=true,
}

@article{firtina_genomic_2016,
  title = {On genomic repeats and reproducibility},
  volume = {32},
  issn = {1367-4803},
  url = {https://doi.org/10.1093/bioinformatics/btw139},
  doi = {10.1093/bioinformatics/btw139},
  abstract = {Results: Here, we present a comprehensive analysis on the reproducibility of computational characterization of genomic variants using high throughput sequencing data. We reanalyzed the same datasets twice, using the same tools with the same parameters, where we only altered the order of reads in the input (i.e. FASTQ file). Reshuffling caused the reads from repetitive regions being mapped to different locations in the second alignment, and we observed similar results when we only applied a scatter/gather approach for read mapping—without prior shuffling. Our results show that, some of the most common variation discovery algorithms do not handle the ambiguous read mappings accurately when random locations are selected. In addition, we also observed that even when the exact same alignment is used, the GATK HaplotypeCaller generates slightly different call sets, which we pinpoint to the variant filtration step. We conclude that, algorithms at each step of genomic variation discovery and characterization need to treat ambiguous mappings in a deterministic fashion to ensure full replication of results.},
  number = {15},
  journal = {Bioinformatics},
  author = {Firtina, Can and Alkan, Can},
  montha={August},
  year = {2016},
  pages = {2243--2247},
  pdf={firtina_2016_on_genomic_repeats_and_reproducibility.pdf},
  code={https://zenodo.org/record/32611#.YmcGgy8RqPw},
  abbr={Bioinformatics},
  bibtex_show=true,
}
